{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating generative output\n",
    "Superpipe can also be used to evaluate generative output. There are two primary ways to evaluate the quality of generative output:\n",
    "1. Assertions\n",
    "2. Side-by-side comparisons\n",
    "\n",
    "In this example, we'll ask GPT-4 to help us with both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superpipe import *\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our pipeline\n",
    "The `SimpleLLMStep` takes in a prompt, model and name and outputs a column with that name. In this case, we just want the LLM to tell us a joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_prompt = lambda row: f\"\"\"\n",
    "Tell me a joke about {row['topic']}\n",
    "\"\"\"\n",
    "\n",
    "JokesStep = steps.SimpleLLMStep(\n",
    "  prompt=joke_prompt,\n",
    "  model=models.gpt35,\n",
    "  name=\"joke\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['Beets', 'Bears', 'Battlestar Gallactica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_df = pd.DataFrame(topics)\n",
    "jokes_df.columns = [\"topic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Battlestar Gallactica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   topic\n",
       "0                  Beets\n",
       "1                  Bears\n",
       "2  Battlestar Gallactica"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we define our pipeline we need to create our evaluation function. We're pulling out the `get_structured_llm_response` function from `Superpipe` to make our lives easier. Our evaluation function just needs to return a boolean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompt(row):\n",
    "    return f\"\"\"\n",
    "    Is the following joke pretty funny? Your bar should be making a friend laugh out loud\n",
    "    {row['joke']}\n",
    "\n",
    "    Return a json object with a single boolean key called 'evaluation'\n",
    "    \"\"\"\n",
    "\n",
    "def evaluate_joke(row):\n",
    "    return llm.get_structured_llm_response(evaluate_prompt(row), models.gpt4).content['evaluation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our simple pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step joke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     True\n",
      "1     True\n",
      "2    False\n",
      "dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>__joke__</th>\n",
       "      <th>joke</th>\n",
       "      <th>__joke4__</th>\n",
       "      <th>joke4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beets</td>\n",
       "      <td>{'input_tokens': 16, 'output_tokens': 17, 'inp...</td>\n",
       "      <td>I tried to make a beet pun, but it just didn't...</td>\n",
       "      <td>{'input_tokens': 16, 'output_tokens': 14, 'inp...</td>\n",
       "      <td>Why did the beet turn red?\\n\\nBecause it saw t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bears</td>\n",
       "      <td>{'input_tokens': 15, 'output_tokens': 13, 'inp...</td>\n",
       "      <td>Why did the bear dissolve in water?\\n\\nBecause...</td>\n",
       "      <td>{'input_tokens': 15, 'output_tokens': 15, 'inp...</td>\n",
       "      <td>Why don't bears wear socks?\\n\\nBecause they li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Battlestar Gallactica</td>\n",
       "      <td>{'input_tokens': 19, 'output_tokens': 29, 'inp...</td>\n",
       "      <td>Why did the Cylon break up with his girlfriend...</td>\n",
       "      <td>{'input_tokens': 19, 'output_tokens': 22, 'inp...</td>\n",
       "      <td>Why did the Cylon buy an iPhone?\\n\\nBecause he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   topic                                           __joke__  \\\n",
       "0                  Beets  {'input_tokens': 16, 'output_tokens': 17, 'inp...   \n",
       "1                  Bears  {'input_tokens': 15, 'output_tokens': 13, 'inp...   \n",
       "2  Battlestar Gallactica  {'input_tokens': 19, 'output_tokens': 29, 'inp...   \n",
       "\n",
       "                                                joke  \\\n",
       "0  I tried to make a beet pun, but it just didn't...   \n",
       "1  Why did the bear dissolve in water?\\n\\nBecause...   \n",
       "2  Why did the Cylon break up with his girlfriend...   \n",
       "\n",
       "                                           __joke4__  \\\n",
       "0  {'input_tokens': 16, 'output_tokens': 14, 'inp...   \n",
       "1  {'input_tokens': 15, 'output_tokens': 15, 'inp...   \n",
       "2  {'input_tokens': 19, 'output_tokens': 22, 'inp...   \n",
       "\n",
       "                                               joke4  \n",
       "0  Why did the beet turn red?\\n\\nBecause it saw t...  \n",
       "1  Why don't bears wear socks?\\n\\nBecause they li...  \n",
       "2  Why did the Cylon buy an iPhone?\\n\\nBecause he...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedian = pipeline.Pipeline([\n",
    "    JokesStep,\n",
    "], evaluation_fn=evaluate_joke)\n",
    "\n",
    "comedian.run(jokes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineStatistics(score=0.6666666666666666, input_tokens=defaultdict(<class 'int'>, {}), output_tokens=defaultdict(<class 'int'>, {}), input_cost=0.0, output_cost=0.0, num_success=0, num_failure=0, total_latency=0.0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedian.statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our eval is working -- it thought 2/3 jokes were funny. \n",
    "\n",
    "Our eval is extremely subjective right now and slight changes to the prompt will move our eval statistic dramatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-by-side evals\n",
    "\n",
    "Side-by-sides are still subjective but are often more aligned with the choice an AI engineer is making. The question is often not \"is this funny\" but rather \"which model is funnier\". \n",
    "\n",
    "Let's create a GPT-4 joke step to compare to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "JokesStep4 = steps.SimpleLLMStep(\n",
    "  prompt=joke_prompt,\n",
    "  model=models.gpt4,\n",
    "  name=\"joke4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an evaluation function that compares the jokes side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_side_by_side_prompt(row):\n",
    "    return f\"\"\"\n",
    "    You are given two jokes. Rate which is funnier:\n",
    "    joke 1: {row['joke']}\n",
    "    joke 2: {row['joke4']}\n",
    "\n",
    "    If joke 1 is funnier or they are similar, return false. If joke 2 is funnier return true.\n",
    "\n",
    "    Return a json object with a single boolean key called 'evaluation'\n",
    "    \"\"\"\n",
    "\n",
    "def evaluate_side_by_side(row):\n",
    "    return llm.get_structured_llm_response(evaluate_side_by_side_prompt(row), models.gpt4).content['evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"I tried to make a beet pun, but it just didn't turnip right.\",\n",
       "        'Why did the beet turn red?\\n\\nBecause it saw the salad dressing!'],\n",
       "       ['Why did the bear dissolve in water?\\n\\nBecause it was polar!',\n",
       "        \"Why don't bears wear socks?\\n\\nBecause they like to walk bear-foot!\"],\n",
       "       ['Why did the Cylon break up with his girlfriend? She kept telling him, \"You can\\'t Frak your way out of every problem!\"',\n",
       "        'Why did the Cylon buy an iPhone?\\n\\nBecause he heard it comes with Siri-usly good voice recognition!']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_df[['joke', 'joke4']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step joke...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step joke4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    True\n",
      "1    True\n",
      "2    True\n",
      "dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>__joke__</th>\n",
       "      <th>joke</th>\n",
       "      <th>__joke4__</th>\n",
       "      <th>joke4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beets</td>\n",
       "      <td>{'input_tokens': 16, 'output_tokens': 22, 'inp...</td>\n",
       "      <td>Why did the beet break up with the turnip? Bec...</td>\n",
       "      <td>{'input_tokens': 16, 'output_tokens': 14, 'inp...</td>\n",
       "      <td>Why did the beet turn red?\\n\\nBecause it saw t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bears</td>\n",
       "      <td>{'input_tokens': 15, 'output_tokens': 11, 'inp...</td>\n",
       "      <td>Why do bears have hairy coats?\\n\\nFur protection!</td>\n",
       "      <td>{'input_tokens': 15, 'output_tokens': 15, 'inp...</td>\n",
       "      <td>Why don't bears wear socks?\\n\\nBecause they li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Battlestar Gallactica</td>\n",
       "      <td>{'input_tokens': 19, 'output_tokens': 18, 'inp...</td>\n",
       "      <td>Why did the Cylon break up with the toaster?\\n...</td>\n",
       "      <td>{'input_tokens': 19, 'output_tokens': 26, 'inp...</td>\n",
       "      <td>Why did the Cylon go to Starbucks?\\n\\nBecause ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   topic                                           __joke__  \\\n",
       "0                  Beets  {'input_tokens': 16, 'output_tokens': 22, 'inp...   \n",
       "1                  Bears  {'input_tokens': 15, 'output_tokens': 11, 'inp...   \n",
       "2  Battlestar Gallactica  {'input_tokens': 19, 'output_tokens': 18, 'inp...   \n",
       "\n",
       "                                                joke  \\\n",
       "0  Why did the beet break up with the turnip? Bec...   \n",
       "1  Why do bears have hairy coats?\\n\\nFur protection!   \n",
       "2  Why did the Cylon break up with the toaster?\\n...   \n",
       "\n",
       "                                           __joke4__  \\\n",
       "0  {'input_tokens': 16, 'output_tokens': 14, 'inp...   \n",
       "1  {'input_tokens': 15, 'output_tokens': 15, 'inp...   \n",
       "2  {'input_tokens': 19, 'output_tokens': 26, 'inp...   \n",
       "\n",
       "                                               joke4  \n",
       "0  Why did the beet turn red?\\n\\nBecause it saw t...  \n",
       "1  Why don't bears wear socks?\\n\\nBecause they li...  \n",
       "2  Why did the Cylon go to Starbucks?\\n\\nBecause ...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedian = pipeline.Pipeline([\n",
    "    JokesStep,\n",
    "    JokesStep4,\n",
    "], evaluation_fn=evaluate_side_by_side)\n",
    "\n",
    "comedian.run(jokes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineStatistics(score=1.0, input_tokens=defaultdict(<class 'int'>, {}), output_tokens=defaultdict(<class 'int'>, {}), input_cost=0.0, output_cost=0.0, num_success=0, num_failure=0, total_latency=0.0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedian.statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like GPT-4 prefers its own jokes! Of course this isn't a rigourous evaluation but it should be enough to get you started evaluating generative outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labelkit-fHKGPBM5-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
